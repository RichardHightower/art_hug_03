{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Setting Up Your Hugging Face Environment\n",
    "\n",
    "This notebook contains all examples from Chapter 3, demonstrating how to set up and use the Hugging Face ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Verification\n",
    "\n",
    "First, let's verify that all required packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.39.3\n",
      "Datasets: 3.6.0\n",
      "Accelerate: 1.8.1\n",
      "PyTorch: 2.7.1\n",
      "HF Hub: 0.33.2\n",
      "\n",
      "Device Information:\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import transformers\n",
    "import datasets\n",
    "import accelerate\n",
    "import torch\n",
    "import huggingface_hub\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"HF Hub:\", huggingface_hub.__version__)\n",
    "\n",
    "# Check device availability\n",
    "print(\"\\nDevice Information:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Pipeline Example\n",
    "\n",
    "HuggingFace pipelines provide a simple API for common NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/richardhightower/src/art_hug_03/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I love HuggingFace!\" -> POSITIVE (score: 1.000)\n",
      "\"This is terrible.\" -> NEGATIVE (score: 1.000)\n",
      "\"The weather is okay today.\" -> POSITIVE (score: 1.000)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test it\n",
    "results = classifier([\n",
    "    \"I love HuggingFace!\",\n",
    "    \"This is terrible.\",\n",
    "    \"The weather is okay today.\"\n",
    "])\n",
    "\n",
    "for text, result in zip([\"I love HuggingFace!\", \"This is terrible.\", \"The weather is okay today.\"], results):\n",
    "    print(f'\"{text}\" -> {result[\"label\"]} (score: {result[\"score\"]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Hub API\n",
    "\n",
    "Explore models available on the HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelFilter\n",
    "\n",
    "# Create an API client\n",
    "api = HfApi()\n",
    "\n",
    "# List text-classification models\n",
    "models = api.list_models(filter=ModelFilter(task=\"text-classification\"))\n",
    "model_list = list(models)\n",
    "\n",
    "print(f\"Found {len(model_list)} text-classification models!\")\n",
    "print(\"\\nTop 5 most downloaded:\")\n",
    "\n",
    "# Show top 5\n",
    "sorted_models = sorted(model_list, key=lambda x: x.downloads or 0, reverse=True)[:5]\n",
    "for i, model in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {model.modelId} (Downloads: {model.downloads:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Download Example\n",
    "\n",
    "Download and use a specific model with tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Download tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model for inference\n",
    "import torch\n",
    "\n",
    "text = \"HuggingFace makes NLP so much easier!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Prediction: Negative={predictions[0][0]:.3f}, Positive={predictions[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Translation Pipeline\n",
    "\n",
    "Demonstrate translation with batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create translation pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_fr\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"Hugging Face makes AI easy.\",\n",
    "    \"Transformers are powerful.\"\n",
    "]\n",
    "\n",
    "# Translate with batch processing\n",
    "translations = translator(sentences, batch_size=2)\n",
    "\n",
    "for original, result in zip(sentences, translations):\n",
    "    print(f\"EN: {original}\")\n",
    "    print(f\"FR: {result['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation\n",
    "\n",
    "Generate text using GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# Generate text\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=2,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\nGenerated continuations:\")\n",
    "for i, generated in enumerate(result, 1):\n",
    "    print(f\"\\n{i}. {generated['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Zero-Shot Classification\n",
    "\n",
    "Classify text without training on specific labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification\n",
    "classifier = pipeline(\"zero-shot-classification\", device=device)\n",
    "\n",
    "text = \"This is a tutorial about natural language processing with transformers.\"\n",
    "candidate_labels = [\"education\", \"politics\", \"entertainment\", \"technology\", \"sports\"]\n",
    "\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(\"\\nClassification scores:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Question Answering\n",
    "\n",
    "Extract answers from context using QA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "\n",
    "context = \"\"\"\n",
    "HuggingFace is a company that develops tools for building applications using machine learning.\n",
    "It is most notable for its Transformers library built for natural language processing applications\n",
    "and its platform that allows users to share machine learning models and datasets.\n",
    "The company was founded in 2016 by Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is HuggingFace?\",\n",
    "    \"When was the company founded?\",\n",
    "    \"Who founded HuggingFace?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (score: {result['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Named Entity Recognition\n",
    "\n",
    "Identify entities in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER pipeline\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. The company is now led by Tim Cook.\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(\"\\nEntities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['word']} -> {entity['entity_group']} (score: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison\n",
    "\n",
    "Compare different models for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sentiment analysis models\n",
    "models = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "]\n",
    "\n",
    "text = \"This product is amazing! I highly recommend it.\"\n",
    "\n",
    "for model_name in models:\n",
    "    try:\n",
    "        classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "        result = classifier(text)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Result: {result[0]}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Error: {str(e)[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing Performance\n",
    "\n",
    "Demonstrate the performance benefits of batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"I love this!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad at all.\",\n",
    "    \"Could be better.\",\n",
    "    \"Absolutely fantastic!\",\n",
    "    \"Waste of time.\",\n",
    "    \"Pretty good overall.\",\n",
    "    \"Highly disappointed.\"\n",
    "]\n",
    "\n",
    "# Single processing\n",
    "start = time.time()\n",
    "single_results = []\n",
    "for text in texts:\n",
    "    result = classifier(text)\n",
    "    single_results.append(result)\n",
    "single_time = time.time() - start\n",
    "\n",
    "# Batch processing\n",
    "start = time.time()\n",
    "batch_results = classifier(texts, batch_size=4)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"Single processing time: {single_time:.3f}s\")\n",
    "print(f\"Batch processing time: {batch_time:.3f}s\")\n",
    "print(f\"Speedup: {single_time/batch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cache Information\n",
    "\n",
    "Check the HuggingFace cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get cache directory\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "\n",
    "if cache_dir.exists():\n",
    "    # Count cached models\n",
    "    model_dirs = [d for d in cache_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    # Calculate total size\n",
    "    total_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "    \n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Number of cached models: {len(model_dirs)}\")\n",
    "    print(f\"Total cache size: {total_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(\"No cache directory found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. HuggingFace Spaces Example\n",
    "\n",
    "Example code for deploying to HuggingFace Spaces with Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Gradio app for HuggingFace Spaces\n",
    "# Note: This is example code - Gradio needs to be installed separately\n",
    "\n",
    "example_gradio_code = '''\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    results = classifier(text)\n",
    "    return {\n",
    "        \"label\": results[0][\"label\"],\n",
    "        \"score\": results[0][\"score\"]\n",
    "    }\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=analyze_sentiment,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Enter text to analyze...\"),\n",
    "    outputs=gr.JSON(),\n",
    "    title=\"Sentiment Analysis Demo\",\n",
    "    description=\"Analyze the sentiment of your text using HuggingFace Transformers\",\n",
    "    examples=[\n",
    "        [\"I love this product!\"],\n",
    "        [\"This is terrible.\"],\n",
    "        [\"It's okay, nothing special.\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()\n",
    "'''\n",
    "\n",
    "print(\"Example Gradio app for HuggingFace Spaces:\")\n",
    "print(\"=\" * 50)\n",
    "print(example_gradio_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the essential components of setting up and using the HuggingFace ecosystem:\n",
    "\n",
    "1. **Environment Verification** - Checking installations\n",
    "2. **Pipelines** - Simple API for common NLP tasks\n",
    "3. **Hub API** - Discovering and exploring models\n",
    "4. **Model Loading** - Downloading and using specific models\n",
    "5. **Various NLP Tasks** - Translation, generation, QA, NER, etc.\n",
    "6. **Performance** - Batch processing benefits\n",
    "7. **Deployment** - Example for HuggingFace Spaces\n",
    "\n",
    "These examples provide a solid foundation for working with HuggingFace Transformers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
