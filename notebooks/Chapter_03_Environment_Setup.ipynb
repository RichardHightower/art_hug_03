{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Setting Up Your Hugging Face Environment\n",
    "\n",
    "This notebook contains all examples from Chapter 3, demonstrating how to set up and use the Hugging Face ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Verification\n",
    "\n",
    "First, let's verify that all required packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.39.3\n",
      "Datasets: 3.6.0\n",
      "Accelerate: 1.8.1\n",
      "PyTorch: 2.7.1\n",
      "HF Hub: 0.33.2\n",
      "\n",
      "Device Information:\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import transformers\n",
    "import datasets\n",
    "import accelerate\n",
    "import torch\n",
    "import huggingface_hub\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"HF Hub:\", huggingface_hub.__version__)\n",
    "\n",
    "# Check device availability\n",
    "print(\"\\nDevice Information:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Pipeline Example\n",
    "\n",
    "HuggingFace pipelines provide a simple API for common NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/richardhightower/src/art_hug_03/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I love HuggingFace!\" -> POSITIVE (score: 1.000)\n",
      "\"This is terrible.\" -> NEGATIVE (score: 1.000)\n",
      "\"The weather is okay today.\" -> POSITIVE (score: 1.000)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test it\n",
    "results = classifier([\n",
    "    \"I love HuggingFace!\",\n",
    "    \"This is terrible.\",\n",
    "    \"The weather is okay today.\"\n",
    "])\n",
    "\n",
    "for text, result in zip([\"I love HuggingFace!\", \"This is terrible.\", \"The weather is okay today.\"], results):\n",
    "    print(f'\"{text}\" -> {result[\"label\"]} (score: {result[\"score\"]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Hub API\n",
    "\n",
    "Explore models available on the HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 text-classification models!\n",
      "\n",
      "Top 5 most downloaded:\n",
      "1. cardiffnlp/twitter-roberta-base-sentiment-latest (Downloads: 4,556,383)\n",
      "2. cross-encoder/ms-marco-MiniLM-L6-v2 (Downloads: 4,336,948)\n",
      "3. distilbert/distilbert-base-uncased-finetuned-sst-2-english (Downloads: 3,504,257)\n",
      "4. facebook/bart-large-mnli (Downloads: 2,590,462)\n",
      "5. BAAI/bge-reranker-v2-m3 (Downloads: 2,363,490)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Create an API client\n",
    "api = HfApi()\n",
    "\n",
    "# List text-classification models\n",
    "models = list(api.list_models(task=\"text-classification\", limit=100))\n",
    "\n",
    "print(f\"Found {len(models)} text-classification models!\")\n",
    "print(\"\\nTop 5 most downloaded:\")\n",
    "\n",
    "# Show top 5\n",
    "sorted_models = sorted(models, key=lambda x: x.downloads or 0, reverse=True)[:5]\n",
    "for i, model in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {model.modelId} (Downloads: {model.downloads:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Download Example\n",
    "\n",
    "Download and use a specific model with tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased-finetuned-sst-2-english\n",
      "Model type: distilbert\n",
      "Number of parameters: 66,955,010\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Download tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'HuggingFace makes NLP so much easier!'\n",
      "Prediction: Negative=0.006, Positive=0.994\n"
     ]
    }
   ],
   "source": [
    "# Use the model for inference\n",
    "import torch\n",
    "\n",
    "text = \"HuggingFace makes NLP so much easier!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Prediction: Negative={predictions[0][0]:.3f}, Positive={predictions[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Translation Pipeline\n",
    "\n",
    "Demonstrate translation with batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhightower/src/art_hug_03/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: Hugging Face makes AI easy.\n",
      "FR: Le visage hugging rend l'IA facile.\n",
      "\n",
      "EN: Transformers are powerful.\n",
      "FR: Les transformateurs sont puissants.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create translation pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_fr\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"Hugging Face makes AI easy.\",\n",
    "    \"Transformers are powerful.\"\n",
    "]\n",
    "\n",
    "# Translate with batch processing\n",
    "translations = translator(sentences, batch_size=2)\n",
    "\n",
    "for original, result in zip(sentences, translations):\n",
    "    print(f\"EN: {original}\")\n",
    "    print(f\"FR: {result['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation\n",
    "\n",
    "Generate text using GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The future of artificial intelligence is'\n",
      "\n",
      "Generated continuations:\n",
      "\n",
      "1. The future of artificial intelligence is not just a matter of time – it's the future of technology.\"\n",
      "\n",
      "2. The future of artificial intelligence is uncertain. We’re moving towards AI with a goal of achieving the goal of becoming an artificial intelligence: to create a world where computers aren't like computers. This is not just something the world's leaders have been\n"
     ]
    }
   ],
   "source": [
    "# Text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# Generate text\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=2,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\nGenerated continuations:\")\n",
    "for i, generated in enumerate(result, 1):\n",
    "    print(f\"\\n{i}. {generated['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Zero-Shot Classification\n",
    "\n",
    "Classify text without training on specific labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'This is a tutorial about natural language processing with transformers.'\n",
      "\n",
      "Classification scores:\n",
      "  technology: 0.920\n",
      "  education: 0.051\n",
      "  entertainment: 0.017\n",
      "  sports: 0.009\n",
      "  politics: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot classification\n",
    "classifier = pipeline(\"zero-shot-classification\", device=device)\n",
    "\n",
    "text = \"This is a tutorial about natural language processing with transformers.\"\n",
    "candidate_labels = [\"education\", \"politics\", \"entertainment\", \"technology\", \"sports\"]\n",
    "\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(\"\\nClassification scores:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Question Answering\n",
    "\n",
    "Extract answers from context using QA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfaaa73fe4943579f855feb833945b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c056ccf9bc234bf99c4073f73e8b2bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f13774bffc4ad08104a6dcf5ea26e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4789d46f164846cfafdb3409f2790ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e0f5fda964f37b840970d67108195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is HuggingFace?\n",
      "A: a company that develops tools for building applications using machine learning (score: 0.602)\n",
      "\n",
      "Q: When was the company founded?\n",
      "A: 2016 (score: 0.982)\n",
      "\n",
      "Q: Who founded HuggingFace?\n",
      "A: Clément Delangue, Julien Chaumond, and Thomas Wolf (score: 0.842)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "\n",
    "context = \"\"\"\n",
    "HuggingFace is a company that develops tools for building applications using machine learning.\n",
    "It is most notable for its Transformers library built for natural language processing applications\n",
    "and its platform that allows users to share machine learning models and datasets.\n",
    "The company was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is HuggingFace?\",\n",
    "    \"When was the company founded?\",\n",
    "    \"Who founded HuggingFace?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (score: {result['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Named Entity Recognition\n",
    "\n",
    "Identify entities in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb0ff1b5ab44d038b87d85c6e8dd95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba1005b9b86480695715afa6565a203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d761136e543dda961a2195c6e8825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd83b82a234f7c9e7b3b84662ae289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Apple Inc. was founded by Steve Jobs in Cupertino, California. The company is now led by Tim Cook.'\n",
      "\n",
      "Entities found:\n",
      "  Apple Inc -> ORG (score: 1.000)\n",
      "  Steve Jobs -> PER (score: 0.993)\n",
      "  Cupertino -> LOC (score: 0.977)\n",
      "  California -> LOC (score: 0.999)\n",
      "  Tim Cook -> PER (score: 1.000)\n"
     ]
    }
   ],
   "source": [
    "# NER pipeline\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. The company is now led by Tim Cook.\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(\"\\nEntities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['word']} -> {entity['entity_group']} (score: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison\n",
    "\n",
    "Compare different models for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "Result: {'label': 'POSITIVE', 'score': 0.9998874664306641}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3252fbee52146fba9d33c04863258eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7940e3a037804b0199742751073a5c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645ea1db185f4114a0b09d08bf899d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f96ed7f60d540d690deded3afbb0459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064d573a1cb0448699d4db96ef0cc3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: nlptown/bert-base-multilingual-uncased-sentiment\n",
      "Result: {'label': '5 stars', 'score': 0.9384052157402039}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare sentiment analysis models\n",
    "models = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "]\n",
    "\n",
    "text = \"This product is amazing! I highly recommend it.\"\n",
    "\n",
    "for model_name in models:\n",
    "    try:\n",
    "        classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "        result = classifier(text)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Result: {result[0]}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Error: {str(e)[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing Performance\n",
    "\n",
    "Demonstrate the performance benefits of batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single processing time: 0.157s\n",
      "Batch processing time: 0.032s\n",
      "Speedup: 4.90x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"I love this!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad at all.\",\n",
    "    \"Could be better.\",\n",
    "    \"Absolutely fantastic!\",\n",
    "    \"Waste of time.\",\n",
    "    \"Pretty good overall.\",\n",
    "    \"Highly disappointed.\"\n",
    "]\n",
    "\n",
    "# Single processing\n",
    "start = time.time()\n",
    "single_results = []\n",
    "for text in texts:\n",
    "    result = classifier(text)\n",
    "    single_results.append(result)\n",
    "single_time = time.time() - start\n",
    "\n",
    "# Batch processing\n",
    "start = time.time()\n",
    "batch_results = classifier(texts, batch_size=4)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"Single processing time: {single_time:.3f}s\")\n",
    "print(f\"Batch processing time: {batch_time:.3f}s\")\n",
    "print(f\"Speedup: {single_time/batch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cache Information\n",
    "\n",
    "Check the HuggingFace cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: /Users/richardhightower/.cache/huggingface/hub\n",
      "Number of cached models: 22\n",
      "Total cache size: 19.59 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get cache directory\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "\n",
    "if cache_dir.exists():\n",
    "    # Count cached models\n",
    "    model_dirs = [d for d in cache_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    # Calculate total size\n",
    "    total_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "    \n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Number of cached models: {len(model_dirs)}\")\n",
    "    print(f\"Total cache size: {total_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(\"No cache directory found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. HuggingFace Spaces Example\n",
    "\n",
    "Example code for deploying to HuggingFace Spaces with Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Gradio app for HuggingFace Spaces:\n",
      "==================================================\n",
      "\n",
      "import gradio as gr\n",
      "from transformers import pipeline\n",
      "\n",
      "# Initialize pipeline\n",
      "classifier = pipeline(\"sentiment-analysis\")\n",
      "\n",
      "def analyze_sentiment(text):\n",
      "    results = classifier(text)\n",
      "    return {\n",
      "        \"label\": results[0][\"label\"],\n",
      "        \"score\": results[0][\"score\"]\n",
      "    }\n",
      "\n",
      "# Create Gradio interface\n",
      "iface = gr.Interface(\n",
      "    fn=analyze_sentiment,\n",
      "    inputs=gr.Textbox(lines=3, placeholder=\"Enter text to analyze...\"),\n",
      "    outputs=gr.JSON(),\n",
      "    title=\"Sentiment Analysis Demo\",\n",
      "    description=\"Analyze the sentiment of your text using HuggingFace Transformers\",\n",
      "    examples=[\n",
      "        [\"I love this product!\"],\n",
      "        [\"This is terrible.\"],\n",
      "        [\"It's okay, nothing special.\"]\n",
      "    ]\n",
      ")\n",
      "\n",
      "# Launch the app\n",
      "iface.launch()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Gradio app for HuggingFace Spaces\n",
    "# Note: This is example code - Gradio needs to be installed separately\n",
    "\n",
    "example_gradio_code = '''\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    results = classifier(text)\n",
    "    return {\n",
    "        \"label\": results[0][\"label\"],\n",
    "        \"score\": results[0][\"score\"]\n",
    "    }\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=analyze_sentiment,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Enter text to analyze...\"),\n",
    "    outputs=gr.JSON(),\n",
    "    title=\"Sentiment Analysis Demo\",\n",
    "    description=\"Analyze the sentiment of your text using HuggingFace Transformers\",\n",
    "    examples=[\n",
    "        [\"I love this product!\"],\n",
    "        [\"This is terrible.\"],\n",
    "        [\"It's okay, nothing special.\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()\n",
    "'''\n",
    "\n",
    "print(\"Example Gradio app for HuggingFace Spaces:\")\n",
    "print(\"=\" * 50)\n",
    "print(example_gradio_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the essential components of setting up and using the HuggingFace ecosystem:\n",
    "\n",
    "1. **Environment Verification** - Checking installations\n",
    "2. **Pipelines** - Simple API for common NLP tasks\n",
    "3. **Hub API** - Discovering and exploring models\n",
    "4. **Model Loading** - Downloading and using specific models\n",
    "5. **Various NLP Tasks** - Translation, generation, QA, NER, etc.\n",
    "6. **Performance** - Batch processing benefits\n",
    "7. **Deployment** - Example for HuggingFace Spaces\n",
    "\n",
    "These examples provide a solid foundation for working with HuggingFace Transformers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
